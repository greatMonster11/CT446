{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dữ liệu được download tại: https://github.com/duyvuleo/VNTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiền xử lý dữ liệu - Các file sau khi xử lý sẽ được lưu lại dưới dạng *.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "dir_path = os.path.join(dir_path, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    dirs = os.listdir(folder_path)\n",
    "    for path in dirs:\n",
    "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
    "        for file_path in tqdm(file_paths):\n",
    "            with open(os.path.join(folder_path, path, file_path), 'r', encoding=\"utf-16\") as f:\n",
    "                lines = f.readlines()\n",
    "                lines = ' '.join(lines)\n",
    "                lines = gensim.utils.simple_preprocess(lines)\n",
    "                lines = ' '.join(lines)\n",
    "                lines = ViTokenizer.tokenize(lines)\n",
    "#                 sentence = ' '.join(words)\n",
    "#                 print(lines)\n",
    "                X.append(lines)\n",
    "                y.append(path)\n",
    "#             break\n",
    "#         break\n",
    "    return X, y\n",
    "\n",
    "train_path = os.path.join(dir_path, 'VNTC/Data/10Topics/Ver1.1/Train_Full')\n",
    "print(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = get_data(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(X_data, open('data/X_data.pkl', 'wb'))\n",
    "pickle.dump(y_data, open('data/y_data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(dir_path, 'VNTC/Data/10Topics/Ver1.1/Test_Full')\n",
    "X_test, y_test = get_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_test, open('data/X_test.pkl', 'wb'))\n",
    "pickle.dump(y_test, open('data/y_test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền sử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_doc(doc):\n",
    "    lines = gensim.utils.simple_preprocess(doc)\n",
    "    lines = ' '.join(lines)\n",
    "    lines = ViTokenizer.tokenize(lines)\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_data = pickle.load(open('data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('data/y_data.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform by SVD to decrease number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
       "             random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_data_n = encoder.fit_transform(y_data)\n",
    "y_test_n = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_class = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
       "       'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
       "       'Vi tinh'], dtype='<U16')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_data, y_data,  X_test=None, y_test=None, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        \n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.8640402843601895\n",
      "Test accuracy:  0.864292378853751\n"
     ]
    }
   ],
   "source": [
    "model = naive_bayes.MultinomialNB()\n",
    "train_model(naive_bayes.MultinomialNB(), X_data_tfidf, y_data, X_test_tfidf, y_test, n_epochs=20, is_neuralnet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with simple Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    layer = Dense(1024, activation='relu')(input_layer)\n",
    "    layer = Dense(1024, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = create_dnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30383 samples, validate on 3376 samples\n",
      "Epoch 1/20\n",
      "30383/30383 [==============================] - 6s 203us/step - loss: 0.2109 - accuracy: 0.9280 - val_loss: 0.2781 - val_accuracy: 0.9085\n",
      "Epoch 2/20\n",
      "30383/30383 [==============================] - 6s 201us/step - loss: 0.1832 - accuracy: 0.9384 - val_loss: 0.2907 - val_accuracy: 0.9061\n",
      "Epoch 3/20\n",
      "30383/30383 [==============================] - 6s 210us/step - loss: 0.1656 - accuracy: 0.9441 - val_loss: 0.2823 - val_accuracy: 0.9061\n",
      "Epoch 4/20\n",
      "30383/30383 [==============================] - 7s 246us/step - loss: 0.1417 - accuracy: 0.9522 - val_loss: 0.2902 - val_accuracy: 0.9120\n",
      "Epoch 5/20\n",
      "30383/30383 [==============================] - 6s 205us/step - loss: 0.1146 - accuracy: 0.9626 - val_loss: 0.3143 - val_accuracy: 0.9064\n",
      "Epoch 6/20\n",
      "30383/30383 [==============================] - 6s 197us/step - loss: 0.0995 - accuracy: 0.9667 - val_loss: 0.3200 - val_accuracy: 0.9079\n",
      "Epoch 7/20\n",
      "30383/30383 [==============================] - 9s 297us/step - loss: 0.0758 - accuracy: 0.9756 - val_loss: 0.3320 - val_accuracy: 0.9114\n",
      "Epoch 8/20\n",
      "30383/30383 [==============================] - 7s 237us/step - loss: 0.0606 - accuracy: 0.9806 - val_loss: 0.3833 - val_accuracy: 0.9079\n",
      "Epoch 9/20\n",
      "30383/30383 [==============================] - 6s 196us/step - loss: 0.0537 - accuracy: 0.9829 - val_loss: 0.3817 - val_accuracy: 0.9046\n",
      "Epoch 10/20\n",
      "30383/30383 [==============================] - 6s 214us/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.3993 - val_accuracy: 0.9117\n",
      "Epoch 11/20\n",
      "30383/30383 [==============================] - 7s 223us/step - loss: 0.0320 - accuracy: 0.9910 - val_loss: 0.4266 - val_accuracy: 0.9105\n",
      "Epoch 12/20\n",
      "30383/30383 [==============================] - 7s 223us/step - loss: 0.0268 - accuracy: 0.9926 - val_loss: 0.4744 - val_accuracy: 0.9028\n",
      "Epoch 13/20\n",
      "30383/30383 [==============================] - 7s 223us/step - loss: 0.0261 - accuracy: 0.9933 - val_loss: 0.4473 - val_accuracy: 0.9055\n",
      "Epoch 14/20\n",
      "30383/30383 [==============================] - 7s 229us/step - loss: 0.0260 - accuracy: 0.9933 - val_loss: 0.4775 - val_accuracy: 0.9020\n",
      "Epoch 15/20\n",
      "30383/30383 [==============================] - 7s 233us/step - loss: 0.0277 - accuracy: 0.9929 - val_loss: 0.4557 - val_accuracy: 0.9076\n",
      "Epoch 16/20\n",
      "30383/30383 [==============================] - 7s 238us/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.4666 - val_accuracy: 0.9129\n",
      "Epoch 17/20\n",
      "30383/30383 [==============================] - 6s 212us/step - loss: 0.0189 - accuracy: 0.9957 - val_loss: 0.4756 - val_accuracy: 0.9067\n",
      "Epoch 18/20\n",
      "30383/30383 [==============================] - 7s 231us/step - loss: 0.0172 - accuracy: 0.9958 - val_loss: 0.4878 - val_accuracy: 0.9085\n",
      "Epoch 19/20\n",
      "30383/30383 [==============================] - 7s 230us/step - loss: 0.0162 - accuracy: 0.9962 - val_loss: 0.5322 - val_accuracy: 0.9070\n",
      "Epoch 20/20\n",
      "30383/30383 [==============================] - 6s 213us/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 0.5022 - val_accuracy: 0.9094\n",
      "Validation accuracy:  0.9093601895734598\n",
      "Test accuracy:  0.9072519008198837\n"
     ]
    }
   ],
   "source": [
    "train_model(classifier=dnn, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, n_epochs=20, is_neuralnet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    \n",
    "    layer = Reshape((10, 30))(input_layer)\n",
    "    layer = Bidirectional(GRU(128, activation='relu', return_sequences=True))(layer)    \n",
    "    layer = Convolution1D(100, 3, activation=\"relu\")(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    classifier.summary()\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 256)           122112    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 8, 100)            76900     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               410112    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 938,734\n",
      "Trainable params: 938,734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30383 samples, validate on 3376 samples\n",
      "Epoch 1/20\n",
      "30383/30383 [==============================] - 17s 562us/step - loss: 1.6820 - accuracy: 0.4021 - val_loss: 0.9410 - val_accuracy: 0.6813\n",
      "Epoch 2/20\n",
      "30383/30383 [==============================] - 14s 469us/step - loss: 0.6440 - accuracy: 0.7965 - val_loss: 0.4903 - val_accuracy: 0.8448\n",
      "Epoch 3/20\n",
      "30383/30383 [==============================] - 14s 447us/step - loss: 0.4552 - accuracy: 0.8574 - val_loss: 0.4096 - val_accuracy: 0.8673\n",
      "Epoch 4/20\n",
      "30383/30383 [==============================] - 15s 501us/step - loss: 0.3863 - accuracy: 0.8764 - val_loss: 0.3975 - val_accuracy: 0.8676\n",
      "Epoch 5/20\n",
      "30383/30383 [==============================] - 14s 468us/step - loss: 0.3483 - accuracy: 0.8887 - val_loss: 0.3337 - val_accuracy: 0.8919\n",
      "Epoch 6/20\n",
      "30383/30383 [==============================] - 15s 486us/step - loss: 0.3278 - accuracy: 0.8939 - val_loss: 0.3252 - val_accuracy: 0.8883\n",
      "Epoch 7/20\n",
      "30383/30383 [==============================] - 15s 484us/step - loss: 0.3024 - accuracy: 0.9012 - val_loss: 0.3232 - val_accuracy: 0.8889\n",
      "Epoch 8/20\n",
      "30383/30383 [==============================] - 16s 512us/step - loss: 0.2902 - accuracy: 0.9031 - val_loss: 0.3051 - val_accuracy: 0.8925\n",
      "Epoch 9/20\n",
      "30383/30383 [==============================] - 16s 529us/step - loss: 0.2809 - accuracy: 0.9066 - val_loss: 0.2999 - val_accuracy: 0.8978\n",
      "Epoch 10/20\n",
      "30383/30383 [==============================] - 14s 477us/step - loss: 0.2710 - accuracy: 0.9096 - val_loss: 0.3206 - val_accuracy: 0.8963\n",
      "Epoch 11/20\n",
      "30383/30383 [==============================] - 14s 471us/step - loss: 0.2604 - accuracy: 0.9132 - val_loss: 0.2875 - val_accuracy: 0.9002\n",
      "Epoch 12/20\n",
      "30383/30383 [==============================] - 16s 510us/step - loss: 0.2462 - accuracy: 0.9187 - val_loss: 0.2952 - val_accuracy: 0.8978\n",
      "Epoch 13/20\n",
      "30383/30383 [==============================] - 14s 471us/step - loss: 0.2335 - accuracy: 0.9214 - val_loss: 0.2755 - val_accuracy: 0.9043\n",
      "Epoch 14/20\n",
      "30383/30383 [==============================] - 14s 471us/step - loss: 0.2251 - accuracy: 0.9251 - val_loss: 0.2897 - val_accuracy: 0.8978\n",
      "Epoch 15/20\n",
      "30383/30383 [==============================] - 15s 478us/step - loss: 0.2155 - accuracy: 0.9276 - val_loss: 0.2791 - val_accuracy: 0.9028\n",
      "Epoch 16/20\n",
      "30383/30383 [==============================] - 15s 491us/step - loss: 0.2054 - accuracy: 0.9306 - val_loss: 0.2770 - val_accuracy: 0.9031\n",
      "Epoch 17/20\n",
      "30383/30383 [==============================] - 15s 493us/step - loss: 0.1929 - accuracy: 0.9360 - val_loss: 0.3012 - val_accuracy: 0.8987\n",
      "Epoch 18/20\n",
      "30383/30383 [==============================] - 15s 504us/step - loss: 0.1874 - accuracy: 0.9384 - val_loss: 0.2797 - val_accuracy: 0.9028\n",
      "Epoch 19/20\n",
      "30383/30383 [==============================] - 17s 543us/step - loss: 0.1758 - accuracy: 0.9402 - val_loss: 0.2771 - val_accuracy: 0.9043\n",
      "Epoch 20/20\n",
      "30383/30383 [==============================] - 20s 653us/step - loss: 0.1697 - accuracy: 0.9421 - val_loss: 0.2859 - val_accuracy: 0.9073\n",
      "Validation accuracy:  0.9072867298578199\n",
      "Test accuracy:  0.9014352927163362\n"
     ]
    }
   ],
   "source": [
    "classifier = create_cnn_model()\n",
    "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with real world document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = '''\n",
    "Không được tiếp cận thông tin dự án, vai trò mờ nhạt, từ chối thanh toán… là những lý do 6 cựu lãnh đạo MobiFone trình bày để xin hưởng án treo.\n",
    "\n",
    "Chiều 23/4, khi bị xét hỏi ở phiên phúc thẩm vụ án xảy ra tại Tổng công ty Viễn thông (MobiFone), sáu cựu phó tổng giám đốc doanh nghiệp này gồm: Phan Thị Hoa Mai, Hồ Tuấn, Phạm Thị Phương Anh, Nguyễn Mạnh Hùng, Nguyễn Bảo Long, Nguyễn Đăng Nguyên đều kháng cáo xin giảm nhẹ hình phạt, hưởng án treo hoặc miễn hình phạt.\n",
    "\n",
    "Bản án sơ thẩm của TAND Hà Nội tuyên phạt 6 người từ 2 năm đến 2 năm 6 tháng tù cùng về tội Vi phạm các quy định về quản lý đầu tư công gây hậu quả nghiêm trọng, theo điều 220 Bộ luật Hình sự 2015\n",
    "\n",
    "Sáu người bị TAND Hà Nội kết tội trong quá trình thực hiện dự án đầu tư dịch vụ truyền hình của MobiFone bằng cách mua cổ phần của Công ty Nghe nhìn Toàn cầu (AVG) đã đồng phạm cùng ông Nguyễn Bắc Son, Trương Minh Tuấn (cựu bộ trưởng Thông tin và Truyền thông), Lê Nam Trà (cựu chủ tịch MobiFone), Cao Duy Hải (cựu tổng giám đốc MobiFone) gây thiệt hại cho nhà nước gần 6.600 tỷ đồng.\n",
    "\n",
    "Như phiên sơ thẩm, chiều nay 6 người đều khẳng định \"có vai trò mờ nhạt\" trong dự án. Họ không có đủ thông tin, không đủ chuyên môn hoặc thiếu hiểu biết nên vô tình sai phạm. Họ không tư lợi.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = preprocessing_doc(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30000)\n"
     ]
    }
   ],
   "source": [
    "test_doc_tfidf = tfidf_vect.transform([test_doc])\n",
    "print(np.shape(test_doc_tfidf))\n",
    "test_doc_svd = svd.transform(test_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = classifier.predict(test_doc_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argsort(probabilities[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely class: Phap luat -- Probability: 0.9971052\n",
      "Second most likely class: Chinh tri Xa hoi -- Probability: 0.002439355\n",
      "Third most likely class: Vi tinh -- Probability: 0.00040336724\n",
      "Fourth most likely class: Doi song -- Probability: 4.0381543e-05\n",
      "Fifth most likely class: Kinh doanh -- Probability: 4.9036325e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Most likely class:\", number_to_class[index[9]], \"-- Probability:\", probabilities[0,index[9]])\n",
    "print(\"Second most likely class:\", number_to_class[index[8]], \"-- Probability:\", probabilities[0,index[8]])\n",
    "print(\"Third most likely class:\", number_to_class[index[7]], \"-- Probability:\", probabilities[0,index[7]])\n",
    "print(\"Fourth most likely class:\", number_to_class[index[6]], \"-- Probability:\", probabilities[0,index[6]])\n",
    "print(\"Fifth most likely class:\", number_to_class[index[5]], \"-- Probability:\", probabilities[0,index[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
