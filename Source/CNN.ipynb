{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dữ liệu được download tại: https://github.com/duyvuleo/VNTC\n",
    "### Mô hình pre-trained được download tại: https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/npthanh/New Volume/CTU/Spring-2020/NLP/Code/Natual-Language-Processing/Data/vi.vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/npthanh/.local/lib/python3.6/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors \n",
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "word2vec_model_path = os.path.join(dir_path, \"data/vi.vec\")\n",
    "print(word2vec_model_path)\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(word2vec_model_path)\n",
    "vocab = w2v.wv.vocab\n",
    "wv = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(word):\n",
    "    return wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = ['giao_thông', 'bóng_đá', 'tài_chính', 'thị_trường', 'sức_khoẻ', 'thế_giới', 'thể_thao', 'ẩm_thực']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_word = ['đường_bộ', 'du_lịch', 'ông', 'đau', 'tai_nạn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_best_topic(list_word, list_topic):\n",
    "    best_score = 0\n",
    "    \n",
    "    for topic in list_topic:\n",
    "        topic_score = 0\n",
    "        for word in list_word:\n",
    "            score = wv.similarity(word, topic)\n",
    "            topic_score += score\n",
    "        \n",
    "        if topic_score > best_score:\n",
    "            best_topic = topic\n",
    "            best_score = topic_score\n",
    "    \n",
    "    return best_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/npthanh/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'giao_thông'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_topic(list_word, topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiền sử lý dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_doc(doc):\n",
    "    lines = gensim.utils.simple_preprocess(doc)\n",
    "    lines = ' '.join(lines)\n",
    "    lines = ViTokenizer.tokenize(lines)\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "X_data = pickle.load(open('data/X_data.pkl', 'rb'))\n",
    "y_data = pickle.load(open('data/y_data.pkl', 'rb'))\n",
    "\n",
    "X_test = pickle.load(open('data/X_test.pkl', 'rb'))\n",
    "y_test = pickle.load(open('data/y_test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level - we choose max number of words equal to 30000 except all words (100k+ words)\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\n",
    "tfidf_vect.fit(X_data) # learn vocabulary and idf from training set\n",
    "X_data_tfidf =  tfidf_vect.transform(X_data)\n",
    "# assume that we don't have test set before\n",
    "X_test_tfidf =  tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform by SVD to decrease number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
       "             random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "svd.fit(X_data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_tfidf_svd = svd.transform(X_data_tfidf)\n",
    "X_test_tfidf_svd = svd.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_data_n = encoder.fit_transform(y_data)\n",
    "y_test_n = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_to_class = encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh',\n",
       "       'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa',\n",
       "       'Vi tinh'], dtype='<U16')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    if is_neuralnet:\n",
    "        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\n",
    "        \n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        val_predictions = val_predictions.argmax(axis=-1)\n",
    "        test_predictions = test_predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        classifier.fit(X_train, y_train)\n",
    "    \n",
    "        train_predictions = classifier.predict(X_train)\n",
    "        val_predictions = classifier.predict(X_val)\n",
    "        test_predictions = classifier.predict(X_test)\n",
    "        \n",
    "    print(\"Validation accuracy: \", metrics.accuracy_score(val_predictions, y_val))\n",
    "    print(\"Test accuracy: \", metrics.accuracy_score(test_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    input_layer = Input(shape=(300,))\n",
    "    \n",
    "    layer = Reshape((10, 30))(input_layer)\n",
    "    layer = Bidirectional(GRU(128, activation='relu', return_sequences=True))(layer)    \n",
    "    layer = Convolution1D(100, 3, activation=\"relu\")(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(512, activation='relu')(layer)\n",
    "    layer = Dense(128, activation='relu')(layer)\n",
    "    \n",
    "    output_layer = Dense(10, activation='softmax')(layer)\n",
    "    \n",
    "    classifier = models.Model(input_layer, output_layer)\n",
    "    classifier.summary()\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 10, 30)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 10, 256)           122112    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 100)            76900     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               410112    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 938,734\n",
      "Trainable params: 938,734\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 30383 samples, validate on 3376 samples\n",
      "Epoch 1/20\n",
      "30383/30383 [==============================] - 15s 504us/step - loss: 1.7034 - accuracy: 0.4000 - val_loss: 1.0104 - val_accuracy: 0.6573\n",
      "Epoch 2/20\n",
      "30383/30383 [==============================] - 14s 466us/step - loss: 0.6226 - accuracy: 0.8038 - val_loss: 0.4558 - val_accuracy: 0.8569\n",
      "Epoch 3/20\n",
      "30383/30383 [==============================] - 13s 423us/step - loss: 0.4316 - accuracy: 0.8623 - val_loss: 0.4083 - val_accuracy: 0.8697\n",
      "Epoch 4/20\n",
      "30383/30383 [==============================] - 14s 474us/step - loss: 0.3679 - accuracy: 0.8797 - val_loss: 0.3571 - val_accuracy: 0.8830\n",
      "Epoch 5/20\n",
      "30383/30383 [==============================] - 14s 470us/step - loss: 0.3542 - accuracy: 0.8848 - val_loss: 0.3387 - val_accuracy: 0.8860\n",
      "Epoch 6/20\n",
      "30383/30383 [==============================] - 15s 502us/step - loss: 0.3245 - accuracy: 0.8924 - val_loss: 0.3424 - val_accuracy: 0.8871\n",
      "Epoch 7/20\n",
      "30383/30383 [==============================] - 15s 487us/step - loss: 0.3111 - accuracy: 0.8970 - val_loss: 0.3222 - val_accuracy: 0.8907\n",
      "Epoch 8/20\n",
      "30383/30383 [==============================] - 14s 474us/step - loss: 0.2918 - accuracy: 0.9031 - val_loss: 0.3160 - val_accuracy: 0.8901\n",
      "Epoch 9/20\n",
      "30383/30383 [==============================] - 14s 464us/step - loss: 0.2819 - accuracy: 0.9057 - val_loss: 0.3022 - val_accuracy: 0.8940\n",
      "Epoch 10/20\n",
      "30383/30383 [==============================] - 14s 464us/step - loss: 0.2642 - accuracy: 0.9126 - val_loss: 0.3092 - val_accuracy: 0.8940\n",
      "Epoch 11/20\n",
      "30383/30383 [==============================] - 14s 474us/step - loss: 0.2576 - accuracy: 0.9132 - val_loss: 0.3071 - val_accuracy: 0.8951\n",
      "Epoch 12/20\n",
      "30383/30383 [==============================] - 14s 477us/step - loss: 0.2468 - accuracy: 0.9184 - val_loss: 0.3155 - val_accuracy: 0.8919\n",
      "Epoch 13/20\n",
      "30383/30383 [==============================] - 14s 471us/step - loss: 0.2358 - accuracy: 0.9200 - val_loss: 0.3153 - val_accuracy: 0.8925\n",
      "Epoch 14/20\n",
      "30383/30383 [==============================] - 14s 465us/step - loss: 0.2359 - accuracy: 0.9201 - val_loss: 0.2920 - val_accuracy: 0.8984\n",
      "Epoch 15/20\n",
      "30383/30383 [==============================] - 14s 458us/step - loss: 0.2147 - accuracy: 0.9285 - val_loss: 0.2949 - val_accuracy: 0.9011\n",
      "Epoch 16/20\n",
      "30383/30383 [==============================] - 14s 458us/step - loss: 0.2040 - accuracy: 0.9314 - val_loss: 0.2986 - val_accuracy: 0.8999\n",
      "Epoch 17/20\n",
      "30383/30383 [==============================] - 14s 472us/step - loss: 0.1980 - accuracy: 0.9331 - val_loss: 0.2947 - val_accuracy: 0.9014\n",
      "Epoch 18/20\n",
      "30383/30383 [==============================] - 14s 461us/step - loss: 0.1873 - accuracy: 0.9358 - val_loss: 0.2924 - val_accuracy: 0.9073\n",
      "Epoch 19/20\n",
      "30383/30383 [==============================] - 14s 467us/step - loss: 0.1782 - accuracy: 0.9393 - val_loss: 0.3043 - val_accuracy: 0.8981\n",
      "Epoch 20/20\n",
      "30383/30383 [==============================] - 15s 507us/step - loss: 0.1722 - accuracy: 0.9413 - val_loss: 0.2945 - val_accuracy: 0.9020\n",
      "Validation accuracy:  0.9019549763033176\n",
      "Test accuracy:  0.9032616679570404\n"
     ]
    }
   ],
   "source": [
    "classifier = create_cnn_model()\n",
    "train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = '''\n",
    "Cơ quan Cảnh sát điều tra (Bộ Công an) ngày 18/3 ra kết luận điều tra, đề nghị truy tố 12 bị can trong vụ án xảy ra tại Ngân hàng TMCP Đầu tư và Phát triển Việt Nam (BIDV) và một số công ty.\n",
    "\n",
    "Kết luận điều tra xác định, ông Hà giữ chức Chủ tịch HĐQT BIDV giai đoạn 2008-2016, đại diện 40% vốn Nhà nước tại ngân hàng này. Khi đương nhiệm, ông Hà đã vi phạm quy chế làm việc của BIDV, có biểu hiện áp đặt, thiếu dân chủ trong chỉ đạo. Ông lấy danh nghĩa BIDV trực tiếp thực hiện các hoạt động xúc tiến đầu tư tại tỉnh Hà Tĩnh cho công ty sân sau trái quy định.\n",
    "\n",
    "Ông Hà chỉ đạo và phê duyệt cấp tín dụng với các điều kiện ưu đãi trong khi hai công ty sân sau là Công ty Cổ phần chăn nuôi Bình Hà và Tập đoàn An Phú không đủ năng lực tài chính về vốn tự có và năng lực thực hiện dự án chăn nuôi bò tại Hà Tĩnh. Lãnh đạo hai công ty sau khi được phê duyệt cho vay và nhận tiền giải ngân đã chiếm đoạt để sử dụng cá nhân hoặc sử dụng vốn vay không đúng mục đích. Dự án liên tục thua lỗ phải dừng hoạt động, gây thiệt hại hơn 1.500 tỷ đồng cho BIDV.\n",
    "\n",
    "Hành vi của ông Hà phạm vào tội Vi phạm quy định về hoạt động ngân hàng, hoạt động khác liên quan đến hoạt động ngân hàng, theo điều 206 Bộ luật Hình sự. Tuy nhiên, tháng 7/2019, ông Hà tử vong về bệnh lý khi đang tạm giam nên Cơ quan cảnh sát điều tra (Bộ Công an) đã đình chỉ điều tra bị can.\n",
    "\n",
    "Các bị can Trần Lục Lang và Đoàn Ánh Sáng (đều là cựu phó tổng giám đốc BIDV), Kiều Đình Hòa (cựu giám đốc BIDV chi nhánh Hà Tĩnh), Lê Thị Vân Anh (cựu trưởng phòng khách hàng doanh nghiệp BIDV chi nhánh Hà Tĩnh) đã đồng phạm với ông Trần Bắc Hà trong việc thẩm định, cấp tín dụng cho Công ty Bình Hà gây thiệt hại cho BIDV hơn 683 tỷ đồng.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = preprocessing_doc(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 30000)\n"
     ]
    }
   ],
   "source": [
    "test_doc_tfidf = tfidf_vect.transform([test_doc])\n",
    "print(np.shape(test_doc_tfidf))\n",
    "test_doc_svd = svd.transform(test_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = classifier.predict(test_doc_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argsort(probabilities[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely class: Phap luat -- Probability: 0.9946531\n",
      "Second most likely class: Chinh tri Xa hoi -- Probability: 0.0046064435\n",
      "Third most likely class: Kinh doanh -- Probability: 0.00055128935\n",
      "Fourth most likely class: Doi song -- Probability: 0.00012419866\n",
      "Fifth most likely class: The thao -- Probability: 4.5744415e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Most likely class:\", number_to_class[index[9]], \"-- Probability:\", probabilities[0,index[9]])\n",
    "print(\"Second most likely class:\", number_to_class[index[8]], \"-- Probability:\", probabilities[0,index[8]])\n",
    "print(\"Third most likely class:\", number_to_class[index[7]], \"-- Probability:\", probabilities[0,index[7]])\n",
    "print(\"Fourth most likely class:\", number_to_class[index[6]], \"-- Probability:\", probabilities[0,index[6]])\n",
    "print(\"Fifth most likely class:\", number_to_class[index[5]], \"-- Probability:\", probabilities[0,index[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('CNN_Text_Classifier.h5') # Save model for using later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
